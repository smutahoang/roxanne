//TwitterLDA with background topic
package roxanne.twittertopicmodels;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Random;

import com.google.gson.JsonObject;
import com.google.gson.JsonParser;

import roxanne.twittertopicmodels.Configure;
import roxanne.twittertopicmodels.utils.RankingUtils;
import roxanne.twittertopicmodels.utils.SimilarityUtils;
import roxanne.twittertopicmodels.utils.TweetPreprocessingUtils;

public class TwitterLDA {
	// tweets aggregation type
	enum TweetAggregation {
		PUBLISHED_TIME, AUTHOR
	}

	private TweetAggregation aggregationType = TweetAggregation.AUTHOR;

	private long startTime = 1485554401000L;
	private long TIME_STEP_WIDTH = 30 * 60 * 1000;// 30 mins
	//
	private String dataPath;
	private String outputPath;
	private HashMap<String, String> allRawTweets;

	private int nTopics;

	// data-preporocessing
	private int Term_Min_NTweets = 5;
	private int Tweet_Min_NTerms = 3;
	private int Document_Min_NTweets = 10;

	private int burningPeriod;
	private int maxIteration;
	private int samplingGap;

	private Random rand;
	// hyperparameters

	private double alpha; // prior for documents' topic distribution
	private double sum_alpha;

	private double beta; // prior for topics' word distribution
	private double sum_beta;
	private double[] gamma;
	private double sum_gamma;
	// data
	private Document[] documents;

	private String[] tweetVocabulary;
	// parameters
	private double[][] tweetTopics;
	private double[] backgroundTopic;
	private double[] coinBias;

	// Gibbs sampling variables
	// epoch - topic count
	private int[][] n_zd; // n_zu[k,d]: number of times topic z is observed in
							// tweets of document d
	private int[] sum_nzd;// sum_nzu[d]: total number of topics that are
							// observed in tweets of document d
	// topic - word count
	private int[][] n_wz;// n_wz[w,z]: number of times word w is generated by a
							// topic z in all tweets
	private int[] sum_nwz;// sum_nw[z]: total number of words that are generated
							// by a topic z in tweets
	private int[] n_wb;// n_wz[w]: number of times word w is generated by a
						// background topic
	private int sum_nwb;// sum_nw[z]: total number of words that are generated
						// by background topic

	// topic - coin count
	private int[] n_c;// sum_nw[c]: total number of words that are
						// associated with coin c
	private int sum_nc;

	// variables for storing statistics over all samples
	// will be used for computing documents' topic distribution and topics' word
	// distribution
	private int[][] final_n_zd;
	private int[] final_sum_nzd;
	private int[][] final_n_wz;
	private int[] final_sum_nwz;
	private int[] final_n_wb;
	private int final_sum_nwb;
	private int[] final_n_c;
	private int final_sum_nc;

	private double tweetLogLikelidhood;

	private HashMap<String, Integer> tweetId2EpochIndex;
	private HashMap<String, Integer> tweetId2TweetIndex;
	private HashMap<Integer, List<String>> topicTopTweets;

	public TwitterLDA(String _dataPath, String _outputPath, int _nTopics) {
		dataPath = _dataPath;
		outputPath = _outputPath;
		nTopics = _nTopics;

		burningPeriod = 100;
		maxIteration = 500;
		samplingGap = 20;

	}

	/***
	 * in case of aggregating tweets by published time
	 * 
	 * @param time
	 * @return
	 */
	private int getEpoch(long time) {
		int t = (int) ((time - startTime) / TIME_STEP_WIDTH);
		if (t < 0)
			t = 0;
		return t;
	}

	/***
	 * 
	 * @return
	 */

	public void readData() {
		try {

			allRawTweets = new HashMap<String, String>();
			HashMap<String, Integer> wordNTweets = new HashMap<String, Integer>();
			HashMap<Integer, HashMap<String, List<String>>> rawDocuments = new HashMap<Integer, HashMap<String, List<String>>>();

			JsonParser parser = new JsonParser();
			SimpleDateFormat tweetDateTimeFormater = new SimpleDateFormat("EEE MMM dd HH:mm:ss +0000 yyyy");
			TweetPreprocessingUtils preprocessingUtils = new TweetPreprocessingUtils();

			List<String> userNames = new ArrayList<String>();
			HashMap<String, Integer> userId2Index = new HashMap<String, Integer>();
			int nTweets = 0;

			File[] data_files = (new File(dataPath)).listFiles();
			for (File file : data_files) {
				BufferedReader br = new BufferedReader(new FileReader(file.getAbsolutePath()));
				String line = null;
				while ((line = br.readLine()) != null) {
					JsonObject jsonTweet = (JsonObject) parser.parse(line);
					String tweetId = jsonTweet.get("id_str").getAsString();
					String userId = ((JsonObject) jsonTweet.get("user")).get("id").getAsString();
					String userName = ((JsonObject) jsonTweet.get("user")).get("screen_name").getAsString();

					String text = jsonTweet.get("text").getAsString();
					text = text.replace('\n', ' ').replace('\r', ' ');

					long createdAt = tweetDateTimeFormater.parse(jsonTweet.get("created_at").getAsString()).getTime();

					int documentId = -1;

					if (aggregationType == TweetAggregation.PUBLISHED_TIME) {
						documentId = getEpoch(createdAt);
					} else if (aggregationType == TweetAggregation.AUTHOR) {
						if (userId2Index.containsKey(userId)) {
							documentId = userId2Index.get(userId);
						} else {
							documentId = userId2Index.size();
							userId2Index.put(userId, documentId);
							userNames.add(userName);
						}
					} else {
						// TO-DO: to define other ways for tweet aggregation
						System.out.println("aggregationType = " + aggregationType + " is not defined!");
						System.exit(-1);
					}

					allRawTweets.put(tweetId, text);
					List<String> terms = preprocessingUtils.extractTermInTweet(text);
					for (int i = 0; i < terms.size(); i++) {
						String word = terms.get(i);
						if (wordNTweets.containsKey(word)) {
							wordNTweets.put(word, 1 + wordNTweets.get(word));
						} else {
							wordNTweets.put(word, 1);
						}
					}
					HashMap<String, List<String>> tweets = rawDocuments.get(documentId);
					if (tweets != null) {
						tweets.put(tweetId, terms);
					} else {
						tweets = new HashMap<String, List<String>>();
						tweets.put(tweetId, terms);
						rawDocuments.put(documentId, tweets);
					}
					nTweets++;
				}
				br.close();
			}
			// filter less frequent words and short tweets

			HashSet<String> removedTerms = new HashSet<String>();
			HashSet<String> removedTweets = new HashSet<String>();
			HashSet<Integer> removedDocuments = new HashSet<Integer>();
			while (true) {
				boolean flag = true;
				for (Map.Entry<Integer, HashMap<String, List<String>>> document : rawDocuments.entrySet()) {
					int d = document.getKey();
					if (removedDocuments.contains(d)) {
						continue;
					}
					nTweets = 0;
					HashMap<String, List<String>> tweets = document.getValue();
					for (Map.Entry<String, List<String>> tweet : tweets.entrySet()) {
						String tId = tweet.getKey();
						if (removedTweets.contains(tId)) {
							continue;
						}
						List<String> terms = tweet.getValue();
						int nTerms = 0;
						for (int i = 0; i < terms.size(); i++) {
							String word = terms.get(i);
							if (removedTerms.contains(word)) {
								continue;
							}
							if (wordNTweets.get(word) >= Term_Min_NTweets) {
								nTerms++;
							} else {
								removedTerms.add(word);
								flag = false;
							}
						}
						if (nTerms < Tweet_Min_NTerms) {// reduce tweet counts
														// of terms in this
														// tweet
							for (int i = 0; i < terms.size(); i++) {
								String word = terms.get(i);
								wordNTweets.put(word, wordNTweets.get(word) - 1);
							}
							removedTweets.add(tId);
							flag = false;
						} else {
							nTweets++;
						}
					}
					if (nTweets < Document_Min_NTweets) {// remove all tweets in this document
						for (Map.Entry<String, List<String>> tweet : tweets.entrySet()) {
							String tId = tweet.getKey();
							if (removedTweets.contains(tId)) {
								continue;
							}
							List<String> terms = tweet.getValue();
							for (int i = 0; i < terms.size(); i++) {
								String word = terms.get(i);
								if (removedTerms.contains(word)) {
									continue;
								}
								wordNTweets.put(word, wordNTweets.get(word) - 1);
							}
						}
						removedDocuments.add(d);
						flag = false;
					}
				}
				if (flag)
					break;
			}

			HashMap<String, Integer> word2Index = new HashMap<String, Integer>();

			documents = new Document[rawDocuments.size() - removedDocuments.size()];
			int documentIndex = 0;
			for (Map.Entry<Integer, HashMap<String, List<String>>> document : rawDocuments.entrySet()) {
				int d = document.getKey();
				if (removedDocuments.contains(d)) {
					continue;
				}
				nTweets = 0;
				HashMap<String, List<String>> allTweets = document.getValue();
				for (Map.Entry<String, List<String>> tweet : allTweets.entrySet()) {
					String tId = tweet.getKey();
					if (removedTweets.contains(tId)) {
						continue;
					}
					nTweets++;
				}
				Tweet[] tweets = new Tweet[nTweets];
				nTweets = 0;
				for (Map.Entry<String, List<String>> tweet : allTweets.entrySet()) {
					String tId = tweet.getKey();
					if (removedTweets.contains(tId)) {
						continue;
					}
					List<String> terms = tweet.getValue();
					int nTerms = 0;
					for (int i = 0; i < terms.size(); i++) {
						String word = terms.get(i);
						if (removedTerms.contains(word)) {
							continue;
						}
						nTerms++;
					}
					int[] words = new int[nTerms];
					nTerms = 0;
					for (int i = 0; i < terms.size(); i++) {
						String word = terms.get(i);
						if (removedTerms.contains(word)) {
							continue;
						}
						if (word2Index.containsKey(word)) {
							words[nTerms] = word2Index.get(word);
						} else {
							words[nTerms] = word2Index.size();
							word2Index.put(word, word2Index.size());
						}
						nTerms++;
					}
					tweets[nTweets] = new Tweet();
					tweets[nTweets].tweetID = tId;
					tweets[nTweets].words = words;
					nTweets++;
				}
				documents[documentIndex] = new Document();
				documents[documentIndex].documentId = d;
				if (aggregationType == TweetAggregation.PUBLISHED_TIME) {

				} else if (aggregationType == TweetAggregation.AUTHOR) {
					documents[documentIndex].documentName = userNames.get(d);
				} else {
					// TO-DO
					System.out.println("aggregationType = " + aggregationType + " is not defined!");
					System.exit(-1);
				}
				documents[documentIndex].tweets = tweets;
				documentIndex++;
			}

			tweetVocabulary = new String[word2Index.size()];
			for (Map.Entry<String, Integer> word : word2Index.entrySet()) {
				tweetVocabulary[word.getValue()] = word.getKey();
			}

		} catch (Exception e) {
			e.printStackTrace();
			System.exit(-1);
		}
	}

	public void outputReformattedData() {
		try {
			BufferedWriter bw_stats = new BufferedWriter(
					new FileWriter(String.format("%s/twiterLDA_stats.csv", outputPath)));
			BufferedWriter bw_tweets = new BufferedWriter(
					new FileWriter(String.format("%s/twiterLDA_tweets.csv", outputPath)));
			for (int i = 0; i < documents.length; i++) {
				Document d = documents[i];
				bw_stats.write(String.format("%d,%d\n", d.documentId, d.tweets.length));
				for (int j = 0; j < d.tweets.length; j++) {
					bw_tweets.write(String.format("%d\t%s", i, d.tweets[j].tweetID));
					for (int w = 0; w < d.tweets[j].words.length; w++) {
						bw_tweets.write(String.format(" %s", tweetVocabulary[d.tweets[j].words[w]]));
					}
					bw_tweets.write("\n");
				}
			}
			bw_stats.close();
			bw_tweets.close();

			bw_stats = new BufferedWriter(new FileWriter(String.format("%s/twiterLDA_vocab.csv", outputPath)));
			for (int i = 0; i < tweetVocabulary.length; i++) {
				bw_stats.write(String.format("%d,%s\n", i, tweetVocabulary[i]));
			}
			bw_stats.close();

		} catch (Exception e) {
			e.printStackTrace();
		}
	}

	private void declareFinalCounts() {
		final_n_zd = new int[nTopics][documents.length];
		final_sum_nzd = new int[documents.length];
		for (int d = 0; d < documents.length; d++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zd[z][d] = 0;
			final_sum_nzd[d] = 0;
		}
		final_n_wz = new int[tweetVocabulary.length][nTopics];
		final_sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] = 0;
			final_sum_nwz[z] = 0;
		}

		final_n_wb = new int[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			final_n_wb[w] = 0;
		final_sum_nwb = 0;

		final_n_c = new int[2];
		for (int c = 0; c < 2; c++) {
			final_n_c[c] = 0;
		}
		final_sum_nc = 0;
	}

	private void initilize() {
		rand = new Random();

		// init coin and topic for each tweet and each behavior
		for (int d = 0; d < documents.length; d++) {
			// tweet
			for (int t = 0; t < documents[d].tweets.length; t++) {
				documents[d].tweets[t].topic = rand.nextInt(nTopics);
				int nWords = documents[d].tweets[t].words.length;
				documents[d].tweets[t].coins = new int[nWords];
				for (int i = 0; i < documents[d].tweets[t].coins.length; i++)
					documents[d].tweets[t].coins[i] = rand.nextInt(2);
			}
		}
		// declare and initiate counting tables
		n_zd = new int[nTopics][documents.length];
		sum_nzd = new int[documents.length];
		for (int d = 0; d < documents.length; d++) {
			for (int z = 0; z < nTopics; z++)
				n_zd[z][d] = 0;
			sum_nzd[d] = 0;
		}
		n_wz = new int[tweetVocabulary.length][nTopics];
		sum_nwz = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				n_wz[w][z] = 0;
			sum_nwz[z] = 0;
		}
		n_wb = new int[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			n_wb[w] = 0;
		sum_nwb = 0;

		n_c = new int[2];
		n_c[0] = 0;
		n_c[1] = 0;
		sum_nc = 0;
		// update counting tables
		for (int d = 0; d < documents.length; d++) {
			// tweet
			for (int t = 0; t < documents[d].tweets.length; t++) {
				int z = documents[d].tweets[t].topic;
				// epoch-topic and community-topic
				n_zd[z][d]++;
				sum_nzd[d]++;
				for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
					int w = documents[d].tweets[t].words[i];
					int c = documents[d].tweets[t].coins[i];
					// coin count
					n_c[c]++;
					sum_nc++;
					if (c == 0) {
						// word - background topic
						n_wb[w]++;
						sum_nwb++;
					} else {
						// word - topic
						n_wz[w][z]++;
						sum_nwz[z]++;
					}

				}
			}
		}
	}

	// sampling
	private void setPriors() {
		// epoch topic prior
		alpha = 50.0 / nTopics;
		sum_alpha = 50;

		// topic tweet word prior
		beta = 0.01;
		sum_beta = 0.01 * tweetVocabulary.length;
		// biased coin prior
		gamma = new double[2];
		gamma[0] = 2;
		gamma[1] = 2;
		sum_gamma = gamma[0] + gamma[1];
	}

	private void sampleTweetTopic(int d, int t) {
		// sample the topic for tweet number t of document number d
		// get current topic
		int currz = documents[d].tweets[t].topic;
		n_zd[currz][d]--;
		sum_nzd[d]--;
		for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
			if (documents[d].tweets[t].coins[i] == 0)
				continue;// do not consider background words
			int w = documents[d].tweets[t].words[i];
			n_wz[w][currz]--;
			sum_nwz[currz]--;
		}
		double sump = 0;
		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			p[z] = (n_zd[z][d] + alpha) / (sum_nzd[d] + sum_alpha);
			for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
				if (documents[d].tweets[t].coins[i] == 0)
					continue;// do not consider background words
				int w = documents[d].tweets[t].words[i];
				p[z] = p[z] * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);
			}
			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}
		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z])
				continue;
			// the topic
			documents[d].tweets[t].topic = z;
			// epoch - topic
			n_zd[z][d]++;
			sum_nzd[d]++;
			// topic - word
			for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
				if (documents[d].tweets[t].coins[i] == 0)
					continue;// do not consider background words
				int w = documents[d].tweets[t].words[i];
				n_wz[w][z]++;
				sum_nwz[z]++;
			}
			return;
		}
		System.out.println("bug in sampleTweetTopic");
		for (int z = 0; z < nTopics; z++) {
			System.out.print(p[z] + " ");
		}
		System.exit(-1);
	}

	private void sampleWordCoin(int d, int t, int i) {
		// sample the coin for the word number i of the tweet number t of epoch
		// number d
		// get current coin
		int currc = documents[d].tweets[t].coins[i];
		// get current word
		int w = documents[d].tweets[t].words[i];
		// get current topic
		int z = documents[d].tweets[t].topic;
		// coin count
		n_c[currc]--;
		sum_nc--;
		if (currc == 0) {
			// word - background topic
			n_wb[w]--;
			sum_nwb--;
		} else {
			// word - topic
			n_wz[w][z]--;
			sum_nwz[z]--;
		}

		// probability of coin 0 given priors and recent counts
		double p_0 = (n_c[0] + gamma[0]) / (sum_nc + sum_gamma);
		// probability of w given coin 0
		p_0 = p_0 * (n_wb[w] + beta) / (sum_nwb + sum_beta);

		// probability of coin 1 given priors and recent counts
		double p_1 = (n_c[1] + gamma[1]) / (sum_nc + sum_gamma);
		// probability of w given coin 1 and topic z
		p_1 = p_1 * (n_wz[w][z] + beta) / (sum_nwz[z] + sum_beta);

		double sump = p_0 + p_1;
		sump = rand.nextDouble() * sump;
		int c = 0;
		if (sump > p_0)
			c = 1;
		// the coin
		documents[d].tweets[t].coins[i] = c;
		// coin count
		n_c[c]++;
		sum_nc++;
		if (c == 0) {
			// word-background topic
			n_wb[w]++;
			sum_nwb++;
		} else {
			// word - topic
			n_wz[w][z]++;
			sum_nwz[z]++;
		}
	}

	private void updateFinalCounts() {
		for (int d = 0; d < documents.length; d++) {
			for (int z = 0; z < nTopics; z++)
				final_n_zd[z][d] += n_zd[z][d];
			final_sum_nzd[d] += sum_nzd[d];
		}

		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				final_n_wz[w][z] += n_wz[w][z];
			final_sum_nwz[z] += sum_nwz[z];
		}

		for (int w = 0; w < tweetVocabulary.length; w++)
			final_n_wb[w] += n_wb[w];
		final_sum_nwb += sum_nwb;

		for (int c = 0; c < 2; c++) {
			final_n_c[c] += n_c[c];
		}
		final_sum_nc += sum_nc;
	}

	private void gibbsSampling() {
		System.out.println("Runing Gibbs sampling");
		System.out.print("Setting prios ...");
		setPriors();
		System.out.println(" Done!");
		declareFinalCounts();
		System.out.print("Initializing ... ");
		initilize();
		System.out.println("... Done!");
		for (int iter = 0; iter < burningPeriod + maxIteration; iter++) {
			System.out.print("iteration " + iter);
			// topic
			for (int d = 0; d < documents.length; d++) {
				for (int t = 0; t < documents[d].tweets.length; t++) {
					sampleTweetTopic(d, t);
				}
			}
			// coin
			for (int d = 0; d < documents.length; d++) {
				for (int t = 0; t < documents[d].tweets.length; t++) {
					for (int i = 0; i < documents[d].tweets[t].words.length; i++)
						sampleWordCoin(d, t, i);
				}
			}

			System.out.println(" done!");
			if (samplingGap <= 0)
				continue;
			if (iter < burningPeriod)
				continue;
			if ((iter - burningPeriod) % samplingGap == 0) {
				updateFinalCounts();
			}
		}
		if (samplingGap <= 0)
			updateFinalCounts();
	}

	// inference
	private void inferingModelParameters() {
		// epoch
		for (int d = 0; d < documents.length; d++) {
			// topic distribution
			documents[d].topicDistribution = new double[nTopics];
			for (int z = 0; z < nTopics; z++) {
				documents[d].topicDistribution[z] = (final_n_zd[z][d] + alpha) / (final_sum_nzd[d] + sum_alpha);
			}
		}
		// topics
		tweetTopics = new double[nTopics][tweetVocabulary.length];
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < tweetVocabulary.length; w++)
				tweetTopics[z][w] = (final_n_wz[w][z] + beta) / (final_sum_nwz[z] + sum_beta);
		}

		// background topics
		backgroundTopic = new double[tweetVocabulary.length];
		for (int w = 0; w < tweetVocabulary.length; w++)
			backgroundTopic[w] = (final_n_wb[w] + beta) / (final_sum_nwb + sum_beta);
		// coin bias
		coinBias = new double[2];
		coinBias[0] = (final_n_c[0] + gamma[0]) / (final_sum_nc + sum_gamma);
		coinBias[1] = (final_n_c[1] + gamma[1]) / (final_sum_nc + sum_gamma);

	}

	private double getTweetLikelihood(int d, int t) {
		// compute likelihood of tweet number t of epoch number e
		double logLikelihood = 0;
		for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
			int w = documents[d].tweets[t].words[i];
			// probability that word i is generated by background topic
			double p_0 = backgroundTopic[w] * coinBias[0];
			// probability that word i is generated by other topics
			double p_1 = 0;
			for (int z = 0; z < nTopics; z++) {
				double p_z = tweetTopics[z][w] * documents[d].topicDistribution[z];
				p_1 = p_1 + p_z;
			}
			p_1 = p_1 * coinBias[1];

			logLikelihood = logLikelihood + Math.log10(p_0 + p_1);
			/*
			 * if (Double.isNaN(logLikelihood)) { System.out.println("p_0 = " + p_0 +
			 * "\tp_1 = " + p_1); // System.exit(-1); }
			 */
		}

		return logLikelihood;
	}

	private double getTweetLikelihood(int d, int t, int z) {
		// compute likelihood of tweet number t of epoch e given the topic
		// z
		if (z >= 0) {
			double logLikelihood = 0;
			for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
				int w = documents[d].tweets[t].words[i];
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w] * coinBias[0];
				// probability that word i is generated by topic z
				double p_1 = tweetTopics[z][w] * coinBias[1];
				logLikelihood = logLikelihood + Math.log10(p_0 + p_1);
			}
			return logLikelihood;
		} else {
			double logLikelihood = 0;
			for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
				int w = documents[d].tweets[t].words[i];
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w];
				logLikelihood = logLikelihood + Math.log10(p_0);
			}
			return logLikelihood;
		}
	}

	private double getTweetSumProbUniqueWord(int d, int t, int z) {
		// compute likelihood of tweet number t of document d given the topic
		// z

		HashSet<Integer> uniqueWords = new HashSet<Integer>();
		for (int i = 0; i < documents[d].tweets[t].words.length; i++) {
			int w = documents[d].tweets[t].words[i];
			uniqueWords.add(w);
		}

		double sum = 0;
		if (z >= 0) {
			for (int w : uniqueWords) {
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w] * coinBias[0];
				// probability that word i is generated by topic z
				double p_1 = tweetTopics[z][w] * coinBias[1];
				sum += p_0 + p_1;

			}
		} else {
			for (int w : uniqueWords) {
				// probability that word i is generated by background topic
				double p_0 = backgroundTopic[w];
				sum += p_0;
			}
		}
		return sum;
	}

	private double getTweetEntropy(int d, int t) {
		double[] prob = new double[nTopics];
		double sum = 0;
		for (int z = 0; z < nTopics; z++) {
			prob[z] = getTweetLikelihood(d, t, z);
			prob[z] = Math.exp(prob[z]);
			sum += prob[z];
		}
		double entropy = 0;
		for (int z = 0; z < nTopics; z++) {
			prob[z] /= sum;
			entropy += prob[z] * Math.log(prob[z]);
		}
		return entropy;
	}

	private void getLikelihoodPerplexity() {
		tweetLogLikelidhood = 0;
		for (int d = 0; d < documents.length; d++) {
			for (int t = 0; t < documents[d].tweets.length; t++) {
				double logLikelihood = getTweetLikelihood(d, t);
				tweetLogLikelidhood += logLikelihood;
			}
		}
	}

	private void inferTweetTopic() {
		for (int d = 0; d < documents.length; d++) {
			for (int t = 0; t < documents[d].tweets.length; t++) {

				double[] posteriors = new double[nTopics];
				for (int z = 0; z < nTopics; z++) {
					posteriors[z] = getTweetLikelihood(d, t, z) + Math.log10(documents[d].topicDistribution[z]);
				}

				documents[d].tweets[t].inferedTopic = -1;// background topic only
				documents[d].tweets[t].inferedLikelihood = Double.NEGATIVE_INFINITY;
				double sum = 0;
				for (int z = 0; z < nTopics; z++) {
					if (posteriors[z] > documents[d].tweets[t].inferedLikelihood) {
						documents[d].tweets[t].inferedLikelihood = posteriors[z];
						documents[d].tweets[t].inferedTopic = z;
					}
					posteriors[z] = Math.pow(10, posteriors[z]);
					sum += posteriors[z];
				}
				documents[d].tweets[t].inferedPosteriorProb = posteriors[documents[d].tweets[t].inferedTopic] / sum;
			}
		}
	}

	private void outputTopicWordDistributions() {
		try {
			String fileName = outputPath + "/topicWordDistributions.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++) {
				bw.write("" + z);
				for (int w = 0; w < tweetVocabulary.length; w++)
					bw.write("," + tweetTopics[z][w]);
				bw.write("\n");
			}
			bw.write("background");
			for (int w = 0; w < tweetVocabulary.length; w++)
				bw.write("," + backgroundTopic[w]);
			bw.write("\n");
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputVocabulary() {
		try {
			String fileName = outputPath + "/words.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int i = 0; i < tweetVocabulary.length; i++) {
				bw.write(String.format("%d,%s\n", i, tweetVocabulary[i]));
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet vocabulary to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputCoinBias() {
		try {
			String fileName = outputPath + "/coinBias.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			bw.write(coinBias[0] + "," + coinBias[1]);
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out coin bias to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTopicTopWords(int k) {
		try {
			String fileName = outputPath + "/topicTopWords.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<Integer> topWords = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topWords = RankingUtils.getIndexTopElements(k, tweetTopics[z]);
				for (int j = topWords.size() - 1; j >= 0; j--) {
					int w = topWords.get(j);
					bw.write(String.format("%s,%f\n", tweetVocabulary[w], tweetTopics[z][w]));
				}
			}

			bw.write("***[[BACKGROUND-TOPIC]]***\n");
			topWords = RankingUtils.getIndexTopElements(k * 2, backgroundTopic);
			for (int j = topWords.size() - 1; j >= 0; j--) {
				int w = topWords.get(j);
				bw.write(String.format("%s,%f\n", tweetVocabulary[w], backgroundTopic[w]));
			}

			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputEmpiricalGlobalTopicDistribution() {
		int[] nTweetsByTopic = new int[nTopics];
		for (int z = 0; z < nTopics; z++) {
			nTweetsByTopic[z] = 0;
		}
		int nAllTweets = 0;
		for (int e = 0; e < documents.length; e++) {
			for (int t = 0; t < documents[e].tweets.length; t++) {
				int z = documents[e].tweets[t].inferedTopic;
				nTweetsByTopic[z]++;
			}
			nAllTweets += documents[e].tweets.length;
		}
		try {
			String fileName = outputPath + "/empiricalGlobalTopicDistribution.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int z = 0; z < nTopics; z++) {
				bw.write(String.format("%d,%f\n", z, (double) nTweetsByTopic[z] / nAllTweets));
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out empirical topic distribution!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTopicTopTweetsByPerplexity(int k) {
		int[] tweetPerTopicCount = new int[nTopics];
		int tweetBackgroundTopicCount = 0;
		for (int z = 0; z < nTopics; z++)
			tweetPerTopicCount[z] = 0;
		for (int e = 0; e < documents.length; e++) {
			for (int t = 0; t < documents[e].tweets.length; t++) {
				if (documents[e].tweets[t].inferedTopic >= 0)
					tweetPerTopicCount[documents[e].tweets[t].inferedTopic]++;
				else
					tweetBackgroundTopicCount++;
			}
		}

		String[][] tweetID = new String[nTopics][];
		int[][] tweetEpochIndex = new int[nTopics][];
		int[][] tweetIndex = new int[nTopics][];

		double[][] perTweetPerplexity = new double[nTopics][];
		for (int z = 0; z < nTopics; z++) {
			tweetID[z] = new String[tweetPerTopicCount[z]];
			perTweetPerplexity[z] = new double[tweetPerTopicCount[z]];
			tweetEpochIndex[z] = new int[tweetPerTopicCount[z]];
			tweetIndex[z] = new int[tweetPerTopicCount[z]];
			tweetPerTopicCount[z] = 0;
		}
		String[] backgroundTweetID = new String[tweetBackgroundTopicCount];
		double[] backgroundTweetPerplexity = new double[tweetBackgroundTopicCount];
		tweetBackgroundTopicCount = 0;

		for (int d = 0; d < documents.length; d++) {
			for (int t = 0; t < documents[d].tweets.length; t++) {
				int z = documents[d].tweets[t].inferedTopic;
				if (z >= 0) {
					tweetID[z][tweetPerTopicCount[z]] = documents[d].tweets[t].tweetID;
					perTweetPerplexity[z][tweetPerTopicCount[z]] = documents[d].tweets[t].inferedLikelihood
							/ documents[d].tweets[t].words.length;
					tweetEpochIndex[z][tweetPerTopicCount[z]] = d;
					tweetIndex[z][tweetPerTopicCount[z]] = t;

					tweetPerTopicCount[z]++;
				} else {
					backgroundTweetID[tweetBackgroundTopicCount] = documents[d].tweets[t].tweetID;
					backgroundTweetPerplexity[tweetBackgroundTopicCount] = documents[d].tweets[t].inferedLikelihood;
					tweetBackgroundTopicCount++;
				}

			}
		}

		try {
			String fileName = outputPath + "/topicTopTweetsByPerplexity.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<Integer> topTweets = null;
			// HashSet<String> topTweetIds = null;
			for (int z = 0; z < nTopics; z++) {
				// topTweetIds = topicTopTweets.get(z);
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topTweets = RankingUtils.getIndexTopElements(k, perTweetPerplexity[z]);
				for (int j = topTweets.size() - 1; j >= 0; j--) {
					int t = topTweets.get(j);
					bw.write(String.format("%s,%f,%s\n", tweetID[z][t], perTweetPerplexity[z][t],
							allRawTweets.get(tweetID[z][t])));
					// topTweetIds.add(tweetID[z][t]);
					tweetId2EpochIndex.put(tweetID[z][t], tweetEpochIndex[z][t]);
					tweetId2TweetIndex.put(tweetID[z][t], tweetIndex[z][t]);
				}
			}
			bw.write("***[[BACKGROUND-TOPIC]]***\n");
			topTweets = RankingUtils.getIndexTopElements(k, backgroundTweetPerplexity);
			for (int j = topTweets.size() - 1; j >= 0; j--) {
				int t = topTweets.get(j);
				bw.write(String.format("%s,%f,%s\n", backgroundTweetID[t], backgroundTweetPerplexity[t],
						allRawTweets.get(backgroundTweetID[t])));
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topic top tweets to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputInferedTweetTopic() {
		try {
			String fileName = outputPath + "/tweetTopic.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}

			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int e = 0; e < documents.length; e++) {
				for (int t = 0; t < documents[e].tweets.length; t++)
					bw.write(String.format("%d\t%s\t%d\t%f\t%f\n", e, documents[e].tweets[t].tweetID,
							documents[e].tweets[t].inferedTopic, documents[e].tweets[t].inferedLikelihood,
							documents[e].tweets[t].inferedPosteriorProb));
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweet topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputTopicTopTweetsSumProbUniqueWords(int k) {
		int[] tweetPerTopicCount = new int[nTopics];

		for (int z = 0; z < nTopics; z++)
			tweetPerTopicCount[z] = 0;
		for (int e = 0; e < documents.length; e++) {
			for (int t = 0; t < documents[e].tweets.length; t++) {
				if (documents[e].tweets[t].inferedTopic >= 0)
					tweetPerTopicCount[documents[e].tweets[t].inferedTopic]++;
			}
		}

		String[][] tweetID = new String[nTopics][];
		int[][] tweetEpochIndex = new int[nTopics][];
		int[][] tweetIndex = new int[nTopics][];
		double[][] perTweetEntropy = new double[nTopics][];
		for (int z = 0; z < nTopics; z++) {
			tweetID[z] = new String[tweetPerTopicCount[z]];
			perTweetEntropy[z] = new double[tweetPerTopicCount[z]];
			tweetEpochIndex[z] = new int[tweetPerTopicCount[z]];
			tweetIndex[z] = new int[tweetPerTopicCount[z]];
			tweetPerTopicCount[z] = 0;
		}

		for (int e = 0; e < documents.length; e++) {
			for (int t = 0; t < documents[e].tweets.length; t++) {
				int z = documents[e].tweets[t].inferedTopic;
				if (z >= 0) {
					tweetID[z][tweetPerTopicCount[z]] = documents[e].tweets[t].tweetID;
					perTweetEntropy[z][tweetPerTopicCount[z]] = getTweetSumProbUniqueWord(e, t, z);
					tweetEpochIndex[z][tweetPerTopicCount[z]] = e;
					tweetIndex[z][tweetPerTopicCount[z]] = t;

					tweetPerTopicCount[z]++;
				}
			}
		}

		try {
			String fileName = outputPath + "/topicTopTweetsBySumProbUniqueWords.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<Integer> topTweets = null;
			for (int z = 0; z < nTopics; z++) {
				List<String> topTweetIds = topicTopTweets.get(z);
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				topTweets = RankingUtils.getIndexTopElements(k, perTweetEntropy[z]);
				for (int j = topTweets.size() - 1; j >= 0; j--) {
					int t = topTweets.get(j);
					bw.write(String.format("%s,%f,%s\n", tweetID[z][t], perTweetEntropy[z][t],
							allRawTweets.get(tweetID[z][t])));
					tweetId2EpochIndex.put(tweetID[z][t], tweetEpochIndex[z][t]);
					tweetId2TweetIndex.put(tweetID[z][t], tweetIndex[z][t]);
					topTweetIds.add(tweetID[z][t]);
				}
			}
			bw.close();
		} catch (Exception exception) {
			System.out.println("Error in writing out tweet topic top tweets to file!");
			exception.printStackTrace();
			System.exit(0);
		}
	}

	private List<String> selectRepresentativeTweets(String[] tweetIds, int k, int z) {
		double[][] weightedSimilarity = new double[tweetIds.length][tweetIds.length];
		for (int i = 0; i < tweetIds.length; i++) {
			int e = tweetId2EpochIndex.get(tweetIds[i]);
			int t = tweetId2TweetIndex.get(tweetIds[i]);
			Tweet tweetA = documents[e].tweets[t];
			for (int j = i; j < tweetIds.length; j++) {
				e = tweetId2EpochIndex.get(tweetIds[j]);
				t = tweetId2TweetIndex.get(tweetIds[j]);
				Tweet tweetB = documents[e].tweets[t];
				weightedSimilarity[i][j] = SimilarityUtils.getTweetSimilarity(tweetA, tweetB, tweetTopics[z]);
				weightedSimilarity[j][i] = weightedSimilarity[i][j];
			}
		}
		HashSet<Integer> representativeTweetPositions = new HashSet<Integer>();
		List<String> representativeTweetIds = new ArrayList<String>();
		double[] bestSimilarity = new double[tweetIds.length];
		for (int i = 0; i < tweetIds.length; i++) {
			bestSimilarity[i] = Double.NEGATIVE_INFINITY;
		}
		while (representativeTweetPositions.size() < k) {
			// find max
			double max_value = Double.NEGATIVE_INFINITY;
			int max_position = -1;
			for (int i = 0; i < tweetIds.length; i++) {
				if (representativeTweetPositions.contains(i))
					continue;
				double addedValue = 0;
				for (int j = 0; j < tweetIds.length; j++) {
					if (representativeTweetPositions.contains(j))
						continue;
					if (j == i) {
						continue;
					}
					if (weightedSimilarity[i][j] > bestSimilarity[j]) {
						addedValue += weightedSimilarity[i][j] - bestSimilarity[j];
					}
				}
				if (addedValue > max_value) {
					max_value = addedValue;
					max_position = i;
				}
			}
			// add into selected set
			representativeTweetPositions.add(max_position);
			representativeTweetIds.add(tweetIds[max_position]);
			// update best similarity
			for (int j = 0; j < tweetIds.length; j++) {
				if (representativeTweetPositions.contains(j))
					continue;
				if (weightedSimilarity[max_position][j] > bestSimilarity[j]) {
					bestSimilarity[j] = weightedSimilarity[max_position][j];
				}
			}
		}
		return representativeTweetIds;
	}

	private List<String> pruneTweets(String[] tweetIds, int k, int z) {
		double[][] weightedSimilarity = new double[tweetIds.length][tweetIds.length];
		for (int i = 0; i < tweetIds.length; i++) {
			int e = tweetId2EpochIndex.get(tweetIds[i]);
			int t = tweetId2TweetIndex.get(tweetIds[i]);
			Tweet tweetA = documents[e].tweets[t];
			for (int j = i; j < tweetIds.length; j++) {
				e = tweetId2EpochIndex.get(tweetIds[j]);
				t = tweetId2TweetIndex.get(tweetIds[j]);
				Tweet tweetB = documents[e].tweets[t];
				weightedSimilarity[i][j] = SimilarityUtils.getTweetSimilarity(tweetA, tweetB);
				weightedSimilarity[j][i] = weightedSimilarity[i][j];
			}
		}
		boolean[] mark = new boolean[tweetIds.length];
		for (int i = 0; i < tweetIds.length; i++) {
			mark[i] = true;
		}
		List<String> representativeTweetIds = new ArrayList<String>();
		for (int i = 0; i < tweetIds.length; i++) {
			if (mark[i] == false) {
				continue;
			}
			representativeTweetIds.add(tweetIds[i]);
			if (representativeTweetIds.size() >= k) {
				break;
			}
			for (int j = i; j < tweetIds.length; j++) {
				if (weightedSimilarity[i][j] >= 0.5) {
					mark[j] = false;
				}
			}

		}
		return representativeTweetIds;
	}

	private void outputTopicRepresentativeTweets(int k) {
		try {
			String fileName = outputPath + "/topicRepresentativeTweets.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			List<String> topTweets = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(String.format("***[[TOPIC-%d]]***\n", z));
				// topTweets = selectRepresentativeTweets(
				// (String[]) (topicTopTweets.get(z).toArray(new
				// String[topicTopTweets.get(z).size()])), k, z);
				topTweets = pruneTweets(
						(String[]) (topicTopTweets.get(z).toArray(new String[topicTopTweets.get(z).size()])), k, z);
				for (int j = 0; j < topTweets.size(); j++) {
					bw.write(String.format("%s\t%s\n", topTweets.get(j), allRawTweets.get(topTweets.get(j))));
				}
			}
			bw.close();
		} catch (Exception exception) {
			System.out.println("Error in writing out tweet topic top tweets to file!");
			exception.printStackTrace();
			System.exit(0);
		}

	}

	private void outputDocumentTopicDistribution() {
		try {
			String fileName = null;
			if (aggregationType == TweetAggregation.PUBLISHED_TIME) {
				fileName = outputPath + "/epochTopicDistributions.csv";
			} else {
				fileName = outputPath + "/userTopicDistributions.csv";
			}
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			for (int d = 0; d < documents.length; d++) {
				bw.write(String.format("%d,%s", documents[d].documentId, documents[d].documentName));
				for (int z = 0; z < nTopics; z++)
					bw.write(String.format(",%f", documents[d].topicDistribution[z]));
				bw.write("\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out document topic distributions to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputLikelihood() {
		try {
			String fileName = outputPath + "/likelihood.csv";
			File file = new File(fileName);
			if (!file.exists()) {
				file.createNewFile();
			}
			BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
			bw.write(String.format("%f", tweetLogLikelidhood));
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out tweets to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void learnModel() {
		readData();
		gibbsSampling();
		inferingModelParameters();
		inferTweetTopic();
		getLikelihoodPerplexity();

		outputVocabulary();
		outputInferedTweetTopic();
		outputTopicWordDistributions();
		outputTopicTopWords(20);

		tweetId2EpochIndex = new HashMap<String, Integer>();
		tweetId2TweetIndex = new HashMap<String, Integer>();
		topicTopTweets = new HashMap<Integer, List<String>>();
		for (int z = 0; z < nTopics; z++) {
			topicTopTweets.put(z, new ArrayList<String>());
		}

		outputTopicTopTweetsSumProbUniqueWords(500);
		outputTopicTopTweetsByPerplexity(200);
		// outputTweetTopicTopTweetsEntropy(200);
		outputTopicRepresentativeTweets(50);

		outputDocumentTopicDistribution();
		outputEmpiricalGlobalTopicDistribution();
		outputLikelihood();
		outputCoinBias();
	}

	public static void main(String[] args) {

		new Configure();

		String dataPath = "C:/Users/hoangth/Downloads/input";
		String outputPath = "C:/Users/hoangth/Downloads/output";
		int nTopics = 10;
		TwitterLDA twitterLDA = new TwitterLDA(dataPath, outputPath, nTopics);
		twitterLDA.readData();
		twitterLDA.learnModel();
	}

}
